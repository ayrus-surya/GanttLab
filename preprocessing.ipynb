{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIR",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFBxsZ7YRtio",
        "outputId": "d060deba-46b5-40a6-f81d-61855c56d07a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DupXLMGXVbOQ",
        "outputId": "3b1d53de-63f7-4976-836d-d72c70a4f9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aSeTZc0TU0Y"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRWH7M7LRa-S",
        "outputId": "ded354c3-a823-4e46-96f7-a672303709d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "docs = {}  #<fileid : filename>\n",
        "terms = {}    #<word : {docid:tf-idf}>\n",
        "count_of_terms_in_doc = {} #<docid:count>   total number of words in each doc\n",
        "term_in_docs = {} #<term:set containing docids>      docs a term appears in\n",
        "\n",
        "number_of_documents = 0    #each row is a doc\n",
        "\n",
        "i = 1\n",
        "for f in os.listdir(\"/gdrive/My Drive/AIR/TelevisionNews/\"):\n",
        "\tdocs[str(i)] = f\n",
        "\ti += 1\n",
        "for f_id in docs:\n",
        "  try:\n",
        "    df = pd.read_csv(\"/gdrive/My Drive/AIR/TelevisionNews/\"+docs[f_id])\n",
        "    data = df[\"Snippet\"]\n",
        "  except:\n",
        "    print(docs[f_id])\n",
        "    continue\n",
        "\n",
        "  for row in range(len(data)):\n",
        "    number_of_documents += 1\n",
        "    nwords = 0\n",
        "    for word in data[row].split():\n",
        "      nwords += 1\n",
        "      lw = lemmatizer.lemmatize(word.lower())\n",
        "      if lw not in terms:\n",
        "        terms[lw]={}\n",
        "      if (f_id+\"_\"+str(row+1)) not in terms[lw]:\n",
        "        terms[lw][f_id+\"_\"+str(row+1)]=0\n",
        "        # print(terms)\n",
        "      terms[lw][f_id+\"_\"+str(row+1)]+=1\n",
        "      \n",
        "      if lw not in term_in_docs:\n",
        "        term_in_docs[lw] = set()\n",
        "      term_in_docs[lw].add(f_id)\n",
        "    \n",
        "    count_of_terms_in_doc[f_id+\"_\"+str(row+1)] = nwords\n",
        "\n",
        "print(len(terms))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN.200910.csv\n",
            "73502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umK0dHY1Ul6w",
        "outputId": "96e7b4dd-3338-4a29-cd86-95f5e2dedf00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(number_of_documents)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpaoLRrYoLQG"
      },
      "source": [
        "for term in terms:\n",
        "  for docid in terms[term]:\n",
        "    tf = (terms[term][docid]/count_of_terms_in_doc[docid])\n",
        "    idf = math.log(number_of_documents/len(term_in_docs[term]))\n",
        "    terms[term][docid] = math.log(1+tf)*idf\n",
        "\n",
        "#docid is a combination of file id and row number seperated by _(underscore)\n",
        "#Finally terms is a dictionary with word as a key and value is another dictionary which has doc_id as key and tf-idf as its value\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVJfWki7Aan"
      },
      "source": [
        "import json \n",
        "with open(\"/gdrive/My Drive/AIR/sample.json\", \"w\") as outfile: \n",
        "\tjson.dump(terms, outfile) \n"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}
