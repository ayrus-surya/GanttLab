{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIR",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFBxsZ7YRtio",
        "outputId": "eaeb1db8-f47f-4674-d592-8f3543735132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DupXLMGXVbOQ",
        "outputId": "47fc3aae-7939-4ebc-e5e0-72deb9a1237b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aSeTZc0TU0Y"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRWH7M7LRa-S",
        "outputId": "ddfaf338-86b7-4bd6-94cd-f083e62096da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stpwrds = set(stopwords.words('english'))\n",
        "docs = {}  #<fileid : filename>\n",
        "terms = {}    #<word : {docid:tf-idf}>\n",
        "count_of_terms_in_doc = {} #<docid:count>   total number of words in each doc\n",
        "term_in_docs = {} #<term:set containing docids>      docs a term appears in\n",
        "\n",
        "number_of_documents = 0    #each row is a doc\n",
        "\n",
        "i = 1\n",
        "for f in os.listdir(\"/gdrive/My Drive/AIR/TelevisionNews/\"):\n",
        "\tdocs[str(i)] = f\n",
        "\ti += 1\n",
        "for f_id in docs:\n",
        "  try:\n",
        "    df = pd.read_csv(\"/gdrive/My Drive/AIR/TelevisionNews/\"+docs[f_id])\n",
        "    data = df[\"Snippet\"]\n",
        "  except:\n",
        "    print(docs[f_id])\n",
        "    continue\n",
        "\n",
        "  for row in range(len(data)):\n",
        "    number_of_documents += 1\n",
        "    nwords = 0\n",
        "    string = data[row].replace(',',' ')\n",
        "    string = string.replace('.', ' ')\n",
        "    string = string.replace('?', ' ')\n",
        "    string = string.replace('!', ' ')\n",
        "    string = string.replace('(', ' ')\n",
        "    string = string.replace(')', ' ')\n",
        "    string = string.replace('[', ' ')\n",
        "    string = string.replace(']', ' ')\n",
        "    string = string.replace(':', ' ')\n",
        "    string = string.replace(';', ' ')\n",
        "    string = string.replace('\"', ' ')\n",
        "    string = string.replace('-', ' ')\n",
        "    string = string.replace('/', ' ')\n",
        "    for word in string.split():\n",
        "      if word in stpwrds:\n",
        "        continue\n",
        "      nwords += 1\n",
        "      lw = lemmatizer.lemmatize(word.lower())\n",
        "      if lw not in terms:\n",
        "        terms[lw]={}\n",
        "      if (f_id+\"_\"+str(row+1)) not in terms[lw]:\n",
        "        terms[lw][f_id+\"_\"+str(row+1)]=0\n",
        "        # print(terms)\n",
        "      terms[lw][f_id+\"_\"+str(row+1)]+=1\n",
        "      \n",
        "      if lw not in term_in_docs:\n",
        "        term_in_docs[lw] = set()\n",
        "      term_in_docs[lw].add(f_id)\n",
        "    \n",
        "    count_of_terms_in_doc[f_id+\"_\"+str(row+1)] = nwords\n",
        "\n",
        "print(len(terms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN.200910.csv\n",
            "41398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umK0dHY1Ul6w",
        "outputId": "e0b821f5-903e-4b95-a82d-9b0a848393ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(number_of_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpaoLRrYoLQG"
      },
      "source": [
        "for term in terms:\n",
        "  for docid in terms[term]:\n",
        "    tf = (terms[term][docid]/count_of_terms_in_doc[docid])\n",
        "    idf = math.log(number_of_documents/len(term_in_docs[term]))\n",
        "    terms[term][docid] = (tf,idf)\n",
        "\n",
        "#docid is a combination of file id and row number seperated by _(underscore)\n",
        "#Finally terms is a dictionary with word as a key and value is another dictionary which has doc_id as key and tf-idf as its value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVJfWki7Aan"
      },
      "source": [
        "import json \n",
        "with open(\"/gdrive/My Drive/AIR/output.json\", \"w\") as outfile: \n",
        "\tjson.dump(terms, outfile) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGz4QydDEinA"
      },
      "source": [
        "# count=0\n",
        "# for term in terms:\n",
        "#   if not term.isalpha(): \n",
        "#     count+=1\n",
        "#     print(term)\n",
        "# print(count)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}