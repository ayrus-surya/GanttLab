{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIR",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFBxsZ7YRtio",
        "outputId": "777a8e2b-5c5a-43ec-b0aa-0b4758b7eb37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DupXLMGXVbOQ",
        "outputId": "5bce9ce0-878b-4837-a955-3554986a6065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aSeTZc0TU0Y"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRWH7M7LRa-S",
        "outputId": "dcf79fbc-a65c-42d0-a2d6-55c6cb0ca695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stpwrds = set(stopwords.words('english'))\n",
        "docs = {}  #<fileid : filename>\n",
        "terms = {}    #<word : {docid:tf-idf}>\n",
        "count_of_terms_in_doc = {} #<docid:count>   total number of words in each doc\n",
        "term_in_docs = {} #<term:set containing docids>      docs a term appears in\n",
        "\n",
        "number_of_documents = 0    #each row is a doc\n",
        "\n",
        "i = 1\n",
        "for f in os.listdir(\"/gdrive/My Drive/AIR/TelevisionNews/\"):\n",
        "\tdocs[str(i)] = f\n",
        "\ti += 1\n",
        "for f_id in docs:\n",
        "  try:\n",
        "    df = pd.read_csv(\"/gdrive/My Drive/AIR/TelevisionNews/\"+docs[f_id])\n",
        "    data = df[\"Snippet\"]\n",
        "  except:\n",
        "    print(docs[f_id])\n",
        "    continue\n",
        "\n",
        "  for row in range(len(data)):\n",
        "    number_of_documents += 1\n",
        "    nwords = 0\n",
        "    string = data[row].replace(',',' ')\n",
        "    string = string.replace('.', ' ')\n",
        "    string = string.replace('?', ' ')\n",
        "    string = string.replace('!', ' ')\n",
        "    string = string.replace('(', ' ')\n",
        "    string = string.replace(')', ' ')\n",
        "    string = string.replace('[', ' ')\n",
        "    string = string.replace(']', ' ')\n",
        "    string = string.replace(':', ' ')\n",
        "    string = string.replace(';', ' ')\n",
        "    string = string.replace('\"', ' ')\n",
        "    string = string.replace('-', ' ')\n",
        "    string = string.replace('/', ' ')\n",
        "    for word in string.split():\n",
        "      if word in stpwrds:\n",
        "        continue\n",
        "      nwords += 1\n",
        "      lw = lemmatizer.lemmatize(word.lower())\n",
        "      if lw not in terms:\n",
        "        terms[lw]={}\n",
        "      if (f_id+\"_\"+str(row+1)) not in terms[lw]:\n",
        "        terms[lw][f_id+\"_\"+str(row+1)]=0\n",
        "        # print(terms)\n",
        "      terms[lw][f_id+\"_\"+str(row+1)]+=1\n",
        "      \n",
        "      if lw not in term_in_docs:\n",
        "        term_in_docs[lw] = set()\n",
        "      term_in_docs[lw].add(f_id)\n",
        "    \n",
        "    count_of_terms_in_doc[f_id+\"_\"+str(row+1)] = nwords\n",
        "\n",
        "print(len(terms))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN.200910.csv\n",
            "41398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umK0dHY1Ul6w",
        "outputId": "d2f98224-f642-42cb-f9cb-717d88f3c0ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(number_of_documents)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpaoLRrYoLQG"
      },
      "source": [
        "for term in terms:\n",
        "  for docid in terms[term]:\n",
        "    tf = (terms[term][docid]/count_of_terms_in_doc[docid])\n",
        "    idf = math.log(number_of_documents/len(term_in_docs[term]))\n",
        "    terms[term][docid] = math.log(1+tf)*idf\n",
        "\n",
        "#docid is a combination of file id and row number seperated by _(underscore)\n",
        "#Finally terms is a dictionary with word as a key and value is another dictionary which has doc_id as key and tf-idf as its value\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVJfWki7Aan"
      },
      "source": [
        "import json \n",
        "with open(\"/gdrive/My Drive/AIR/output.json\", \"w\") as outfile: \n",
        "\tjson.dump(terms, outfile) \n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGz4QydDEinA"
      },
      "source": [
        "# count=0\n",
        "# for term in terms:\n",
        "#   if not term.isalpha(): \n",
        "#     count+=1\n",
        "#     print(term)\n",
        "# print(count)"
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}